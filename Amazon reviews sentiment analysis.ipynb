{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import contractions\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAYUSH\\AppData\\Local\\Temp\\ipykernel_22512\\257221721.py:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(file, sep='\\t', error_bad_lines=False)\n",
      "b'Skipping line 20773: expected 15 fields, saw 22\\nSkipping line 39834: expected 15 fields, saw 22\\nSkipping line 52957: expected 15 fields, saw 22\\nSkipping line 54540: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 80276: expected 15 fields, saw 22\\nSkipping line 96168: expected 15 fields, saw 22\\nSkipping line 96866: expected 15 fields, saw 22\\nSkipping line 98175: expected 15 fields, saw 22\\nSkipping line 112539: expected 15 fields, saw 22\\nSkipping line 119377: expected 15 fields, saw 22\\nSkipping line 120065: expected 15 fields, saw 22\\nSkipping line 124703: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 134024: expected 15 fields, saw 22\\nSkipping line 153938: expected 15 fields, saw 22\\nSkipping line 156225: expected 15 fields, saw 22\\nSkipping line 168603: expected 15 fields, saw 22\\nSkipping line 187002: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 200397: expected 15 fields, saw 22\\nSkipping line 203809: expected 15 fields, saw 22\\nSkipping line 207680: expected 15 fields, saw 22\\nSkipping line 223421: expected 15 fields, saw 22\\nSkipping line 244032: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 270329: expected 15 fields, saw 22\\nSkipping line 276484: expected 15 fields, saw 22\\nSkipping line 304755: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 379449: expected 15 fields, saw 22\\nSkipping line 386191: expected 15 fields, saw 22\\nSkipping line 391811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 414348: expected 15 fields, saw 22\\nSkipping line 414773: expected 15 fields, saw 22\\nSkipping line 417572: expected 15 fields, saw 22\\nSkipping line 419496: expected 15 fields, saw 22\\nSkipping line 430528: expected 15 fields, saw 22\\nSkipping line 442230: expected 15 fields, saw 22\\nSkipping line 450931: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 465377: expected 15 fields, saw 22\\nSkipping line 467685: expected 15 fields, saw 22\\nSkipping line 485055: expected 15 fields, saw 22\\nSkipping line 487220: expected 15 fields, saw 22\\nSkipping line 496076: expected 15 fields, saw 22\\nSkipping line 512269: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 529505: expected 15 fields, saw 22\\nSkipping line 531286: expected 15 fields, saw 22\\nSkipping line 535424: expected 15 fields, saw 22\\nSkipping line 569898: expected 15 fields, saw 22\\nSkipping line 586293: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 593880: expected 15 fields, saw 22\\nSkipping line 599274: expected 15 fields, saw 22\\nSkipping line 607961: expected 15 fields, saw 22\\nSkipping line 612413: expected 15 fields, saw 22\\nSkipping line 615913: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 677580: expected 15 fields, saw 22\\nSkipping line 687191: expected 15 fields, saw 22\\nSkipping line 710819: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 728692: expected 15 fields, saw 22\\nSkipping line 730216: expected 15 fields, saw 22\\nSkipping line 758397: expected 15 fields, saw 22\\nSkipping line 760061: expected 15 fields, saw 22\\nSkipping line 768935: expected 15 fields, saw 22\\nSkipping line 769483: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 822725: expected 15 fields, saw 22\\nSkipping line 823621: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 857041: expected 15 fields, saw 22\\nSkipping line 857320: expected 15 fields, saw 22\\nSkipping line 858565: expected 15 fields, saw 22\\nSkipping line 860629: expected 15 fields, saw 22\\nSkipping line 864033: expected 15 fields, saw 22\\nSkipping line 868673: expected 15 fields, saw 22\\nSkipping line 869189: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 938605: expected 15 fields, saw 22\\nSkipping line 940100: expected 15 fields, saw 22\\nSkipping line 975137: expected 15 fields, saw 22\\nSkipping line 976314: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 985597: expected 15 fields, saw 22\\nSkipping line 990873: expected 15 fields, saw 22\\nSkipping line 991806: expected 15 fields, saw 22\\nSkipping line 1019808: expected 15 fields, saw 22\\nSkipping line 1021526: expected 15 fields, saw 22\\nSkipping line 1023905: expected 15 fields, saw 22\\nSkipping line 1044207: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1084683: expected 15 fields, saw 22\\nSkipping line 1093288: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1136430: expected 15 fields, saw 22\\nSkipping line 1139815: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1179821: expected 15 fields, saw 22\\nSkipping line 1195351: expected 15 fields, saw 22\\nSkipping line 1202007: expected 15 fields, saw 22\\nSkipping line 1224868: expected 15 fields, saw 22\\nSkipping line 1232490: expected 15 fields, saw 22\\nSkipping line 1238697: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1258654: expected 15 fields, saw 22\\nSkipping line 1279948: expected 15 fields, saw 22\\nSkipping line 1294360: expected 15 fields, saw 22\\nSkipping line 1302240: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1413654: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1687095: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1805966: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1892134: expected 15 fields, saw 22\\n'\n",
      "C:\\Users\\AAYUSH\\AppData\\Local\\Temp\\ipykernel_22512\\257221721.py:6: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep='\\t', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path = 'C:/Users/AAYUSH/python classes/amazon sentiment analysis/amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "\n",
    "# Read the compressed TSV file into a DataFrame with 'utf-8' encoding and skip bad lines\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as file:\n",
    "    df = pd.read_csv(file, sep='\\t', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "3           1            2.0          3.0    N                 Y   \n",
       "4           4            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "3  and the shredder was dirty and the bin was par...   \n",
       "4                                         Four Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                    Gorgeous colors and easy to use  2015-08-31  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=df[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body star_rating\n",
       "0                                     Great product.           5\n",
       "1  What's to say about this commodity item except...           5\n",
       "2    Haven't used yet, but I am sure I will like it.           5\n",
       "3  Although this was labeled as &#34;new&#34; the...           1\n",
       "4                    Gorgeous colors and easy to use           4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body star_rating\n",
       "0                                           Great product.           5\n",
       "1        What's to say about this commodity item except...           5\n",
       "2          Haven't used yet, but I am sure I will like it.           5\n",
       "3        Although this was labeled as &#34;new&#34; the...           1\n",
       "4                          Gorgeous colors and easy to use           4\n",
       "...                                                    ...         ...\n",
       "2640249  I can't live anymore whithout my Palm III. But...           4\n",
       "2640250  Although the Palm Pilot is thin and compact it...           4\n",
       "2640251  This book had a lot of great content without b...           4\n",
       "2640252  I am teaching a course in Excel and am using t...           5\n",
       "2640253  A very comprehensive layout of exactly how Vis...           5\n",
       "\n",
       "[2640254 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body star_rating\n",
       "0                                           Great product.           5\n",
       "1        What's to say about this commodity item except...           5\n",
       "2          Haven't used yet, but I am sure I will like it.           5\n",
       "3        Although this was labeled as &#34;new&#34; the...           1\n",
       "4                          Gorgeous colors and easy to use           4\n",
       "...                                                    ...         ...\n",
       "2640249  I can't live anymore whithout my Palm III. But...           4\n",
       "2640250  Although the Palm Pilot is thin and compact it...           4\n",
       "2640251  This book had a lot of great content without b...           4\n",
       "2640252  I am teaching a course in Excel and am using t...           5\n",
       "2640253  A very comprehensive layout of exactly how Vis...           5\n",
       "\n",
       "[2640157 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form two classes and select 50000 reviews randomly from each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAYUSH\\AppData\\Local\\Temp\\ipykernel_22512\\966436848.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt['class'] = dt['star_rating'].map({1: 1, 2: 1, 3: 1, 4: 2, 5: 2})\n"
     ]
    }
   ],
   "source": [
    "#creating a class column\n",
    "dt['class'] = dt['star_rating'].map({1: 1, 2: 1, 3: 1, 4: 2, 5: 2})\n",
    "\n",
    "# Set the number of reviews to select for each rating class\n",
    "num_reviews_per_class = 50000\n",
    "\n",
    "# Create a balanced dataset\n",
    "balanced_dt = dt.groupby('class').sample(n=num_reviews_per_class, random_state=42)\n",
    "\n",
    "# Shuffle the balanced dataset to mix the rating classes\n",
    "balanced_dt = balanced_dt.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No frills, basic paper shredder. I shredded ar...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think the G2 is a great pen so I bought this...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cannot fill this pencil with a full sized piec...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perfect for what I need! I use it for sorting ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This purchase was something of an impulsive sp...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>I have owned an HP printer since 1996, going t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>What a Great pen. Writes smooth and ink is dar...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>I really like this keyboard for use with my me...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>First off, I have to say I love the size of th...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>I have gone through 2 sets of these now.  The ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_body star_rating  class\n",
       "0      No frills, basic paper shredder. I shredded ar...           5    2.0\n",
       "1      I think the G2 is a great pen so I bought this...         5.0    2.0\n",
       "2      Cannot fill this pencil with a full sized piec...           3    1.0\n",
       "3      Perfect for what I need! I use it for sorting ...           5    2.0\n",
       "4      This purchase was something of an impulsive sp...           5    2.0\n",
       "...                                                  ...         ...    ...\n",
       "99995  I have owned an HP printer since 1996, going t...           1    1.0\n",
       "99996  What a Great pen. Writes smooth and ink is dar...           5    2.0\n",
       "99997  I really like this keyboard for use with my me...         4.0    2.0\n",
       "99998  First off, I have to say I love the size of th...           3    1.0\n",
       "99999  I have gone through 2 sets of these now.  The ...         2.0    1.0\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before cleaning: 325.75 characters\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'review_body' column to strings\n",
    "balanced_dt['review_body'] = balanced_dt['review_body'].astype(str)\n",
    "\n",
    "# Calculate the average length of reviews before cleaning\n",
    "avg_length_before_datacleaning = balanced_dt['review_body'].apply(len).mean()\n",
    "print(f\"Average length of reviews before cleaning: {avg_length_before_datacleaning:.2f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 0: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 1: Clean HTML\n",
    "    def clean_html(html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for data in soup(['style', 'script', 'code', 'a']):\n",
    "            data.decompose()\n",
    "        return ' '.join(soup.stripped_strings)\n",
    "    \n",
    "    text = clean_html(text)\n",
    "\n",
    "    # Step 2: Remove punctuation using regular expressions\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "    # Step 3: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Step 4: Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 5: Apply contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AAYUSH\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325.75,309.29 \n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to the 'review_body' column\n",
    "balanced_dt['review_body'] = balanced_dt['review_body'].apply(preprocess_text)\n",
    "\n",
    "# Calculate average length of reviews after cleaning\n",
    "avg_length_after_cleaning = balanced_dt['review_body'].apply(len).mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"{avg_length_before_datacleaning:.2f},{avg_length_after_cleaning:.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AAYUSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AAYUSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg length of values before data preprocessing:309.29 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg length of values before data preprocessing:{avg_length_after_cleaning:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dt['review_body'] = balanced_dt['review_body'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dt['review_body'] = balanced_dt['review_body'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length_after_preprocessing = balanced_dt['review_body'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309.29,191.23 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{avg_length_after_cleaning:.2f},{avg_length_after_preprocessing:.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and BoW Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=balanced_dt['review_body']\n",
    "labels=balanced_dt['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(features,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer using count vector\n",
    "vectorizer_01 = CountVectorizer()\n",
    "features_bow_train = vectorizer_01.fit_transform(x_train)\n",
    "features_bow_test = vectorizer_01.transform(x_test)\n",
    "\n",
    "#vectorizer using Tfidf vectorizer\n",
    "vectorizer_02 = TfidfVectorizer()\n",
    "features_tfidf_train = vectorizer_01.fit_transform(x_train)\n",
    "features_tfidf_test = vectorizer_01.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Perceptron Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Precision: 0.77, Recall: 0.83, F1-score: 0.80\n",
      "TF-IDF Precision: 0.77, Recall: 0.83, F1-score: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Initialize Perceptron models\n",
    "perceptron_bow = Perceptron()\n",
    "perceptron_tfidf = Perceptron()\n",
    "\n",
    "# Train the Perceptron models\n",
    "perceptron_bow.fit(features_bow_train, y_train)\n",
    "perceptron_tfidf.fit(features_tfidf_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_bow = perceptron_bow.predict(features_bow_test)\n",
    "y_pred_tfidf = perceptron_tfidf.predict(features_tfidf_test)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-score for BoW and TF-IDF models\n",
    "precision_bow = precision_score(y_test, y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test, y_pred_bow)\n",
    "\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# Print the results in the Jupyter Notebook\n",
    "print(f\"BoW Precision: {precision_bow:.2f}, Recall: {recall_bow:.2f}, F1-score: {f1_bow:.2f}\")\n",
    "print(f\"TF-IDF Precision: {precision_tfidf:.2f}, Recall: {recall_tfidf:.2f}, F1-score: {f1_tfidf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AAYUSH\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Precision: 0.84, Recall: 0.81, F1-score: 0.82\n",
      "TF-IDF Precision: 0.84, Recall: 0.81, F1-score: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AAYUSH\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Perceptron models\n",
    "svm_bow = LinearSVC()\n",
    "svm_tfidf = LinearSVC()\n",
    "\n",
    "# Train the Perceptron models\n",
    "svm_bow.fit(features_bow_train, y_train)\n",
    "svm_tfidf.fit(features_tfidf_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_bow = svm_bow.predict(features_bow_test)\n",
    "y_pred_tfidf = svm_tfidf.predict(features_tfidf_test)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-score for BoW and TF-IDF models\n",
    "precision_bow = precision_score(y_test, y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test, y_pred_bow)\n",
    "\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# Print the results in the Jupyter Notebook\n",
    "print(f\"BoW Precision: {precision_bow:.2f}, Recall: {recall_bow:.2f}, F1-score: {f1_bow:.2f}\")\n",
    "print(f\"TF-IDF Precision: {precision_tfidf:.2f}, Recall: {recall_tfidf:.2f}, F1-score: {f1_tfidf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AAYUSH\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.83      0.84     10002\n",
      "         2.0       0.83      0.86      0.84      9998\n",
      "\n",
      "    accuracy                           0.84     20000\n",
      "   macro avg       0.84      0.84      0.84     20000\n",
      "weighted avg       0.84      0.84      0.84     20000\n",
      "\n",
      "\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.83      0.84     10002\n",
      "         2.0       0.83      0.86      0.84      9998\n",
      "\n",
      "    accuracy                           0.84     20000\n",
      "   macro avg       0.84      0.84      0.84     20000\n",
      "weighted avg       0.84      0.84      0.84     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AAYUSH\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Logistic Regression models\n",
    "logistic_regression_bow = LogisticRegression()\n",
    "logistic_regression_bow.fit(features_bow_train, y_train)\n",
    "\n",
    "logistic_regression_tfidf = LogisticRegression()\n",
    "logistic_regression_tfidf.fit(features_tfidf_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bow = logistic_regression_bow.predict(features_bow_test)\n",
    "y_pred_tfidf = logistic_regression_tfidf.predict(features_tfidf_test)\n",
    "\n",
    "# Classification report for BoW model\n",
    "print(\"BoW Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "\n",
    "# Classification report for TF-IDF model\n",
    "print(\"\\nTF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.76      0.80     10002\n",
      "         2.0       0.78      0.86      0.82      9998\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n",
      "\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.76      0.80     10002\n",
      "         2.0       0.78      0.86      0.82      9998\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train Naive Bayes classifiers\n",
    "naive_bayes_bow = MultinomialNB()\n",
    "naive_bayes_bow.fit(features_bow_train, y_train)\n",
    "\n",
    "naive_bayes_tfidf = MultinomialNB()\n",
    "naive_bayes_tfidf.fit(features_tfidf_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bow = naive_bayes_bow.predict(features_bow_test)\n",
    "y_pred_tfidf = naive_bayes_tfidf.predict(features_tfidf_test)\n",
    "\n",
    "# Classification report for BoW model\n",
    "print(\"BoW Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "\n",
    "# Classification report for TF-IDF model\n",
    "print(\"\\nTF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325.75,309.29 \n",
      "309.29,191.23 \n",
      "BoW Precision: 0.84, Recall: 0.81, F1-score: 0.82\n",
      "TF-IDF Precision: 0.84, Recall: 0.81, F1-score: 0.82\n",
      "BoW Precision: 0.84, Recall: 0.81, F1-score: 0.82\n",
      "TF-IDF Precision: 0.84, Recall: 0.81, F1-score: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(f\"{avg_length_before_datacleaning:.2f},{avg_length_after_cleaning:.2f} \")\n",
    "print(f\"{avg_length_after_cleaning:.2f},{avg_length_after_preprocessing:.2f} \")\n",
    "\n",
    "print(f\"BoW Precision: {precision_bow:.2f}, Recall: {recall_bow:.2f}, F1-score: {f1_bow:.2f}\")\n",
    "print(f\"TF-IDF Precision: {precision_tfidf:.2f}, Recall: {recall_tfidf:.2f}, F1-score: {f1_tfidf:.2f}\")\n",
    "\n",
    "print(f\"BoW Precision: {precision_bow:.2f}, Recall: {recall_bow:.2f}, F1-score: {f1_bow:.2f}\")\n",
    "print(f\"TF-IDF Precision: {precision_tfidf:.2f}, Recall: {recall_tfidf:.2f}, F1-score: {f1_tfidf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.76      0.80     10002\n",
      "         2.0       0.78      0.86      0.82      9998\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n",
      "\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.76      0.80     10002\n",
      "         2.0       0.78      0.86      0.82      9998\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for BoW model\n",
    "print(\"BoW Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "\n",
    "# Classification report for TF-IDF model\n",
    "print(\"\\nTF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.76      0.80     10002\n",
      "         2.0       0.78      0.86      0.82      9998\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n",
      "\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.76      0.80     10002\n",
      "         2.0       0.78      0.86      0.82      9998\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "print(\"\\nTF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
